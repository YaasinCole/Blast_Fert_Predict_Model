{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f01e012-23c4-499a-9aa8-cbf0a740c398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## DATA PREPROCESSING\n",
    "\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('Simulated_Data.csv', sep=\";\" , skiprows=1 , decimal=\",\", encoding=\"utf-8\" , usecols=lambda col: col != 'Patient_ID')\n",
    "df.info()\n",
    "## Dropping null values from the dataset\n",
    "df = df.dropna(how=\"all\")\n",
    "df.isnull().sum()\n",
    "\n",
    "\n",
    "## Presenting data\n",
    "df.describe()\n",
    "\n",
    "## Checking for outliers\n",
    "numeric_df = df.select_dtypes(include=['number'])\n",
    "fig, axs = plt.subplots(len(numeric_df.columns), figsize=(10 , 5 *len(numeric_df.columns)))\n",
    "\n",
    "## Handle the case where there is only one column\n",
    "if len(numeric_df.columns) == 1:\n",
    "    axs = [axs]\n",
    "\n",
    "i = 0\n",
    "for i, col in enumerate(numeric_df.columns):\n",
    "    axs[i].boxplot(numeric_df[col].dropna(), vert=False)\n",
    "    axs[i].set_title(col)\n",
    "    i+=1\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "## After the above runs, we have only two outliers within Total_Sperm_Count (million)  \n",
    "# The approach here is to keep these outliers and see what happens if it skews the model we will either \n",
    "# remove or cap them to the nearest non-outlier value\n",
    "\n",
    "## In the below, we are checking which features correlate with each other\n",
    "corr = numeric_df.corr()\n",
    "plt.figure(figsize=(18, 15), dpi=130)\n",
    "sns.heatmap(numeric_df.corr(), annot=True, fmt= '.2f')\n",
    "plt.savefig(\"correlation_heatmap.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "##Based on the heatmap above, the first test would be to keep all features \n",
    "#Second test we're going to run is dropping one of the features\n",
    "#that are correlated to non-target variable features \n",
    "#This is done before we do any other column manipulation \n",
    "\n",
    "###columns_to_drop = df.drop(columns=[\"Immotile_Sperm (%)\"])\n",
    "\n",
    "## Based on the above heatmap, we can see that there aren't any strong correlations \n",
    "# between any of the features and the target values Fert and Blast, \n",
    "\n",
    "#after the above we then separate the targets from input features also dropping unimportant features from the set\n",
    "x = df.drop(columns=[\"Fertilization Rate (%)\" , \"Blastulation_Rate (%)\" ,\"Seminal_pH\" ,\n",
    "                     \"Viscosity\", \"Total_Sperm_Count (million)\" , \"Sperm_NonProg_Motility (%)\",\n",
    "                    \"Immotile_Sperm (%)\" ,\"Seminal_ORP (mV/106 sperm/ml)\" ,\"SDF (%)\",\n",
    "                    \"SDF (%)\",\t\"CMA3 (%)\",\t\n",
    "\"Viability (Supravital_Staining) (%)\",\t\n",
    "\"Antisperm_Antibodies\",\n",
    "\"Sperm_Agglutination\",\n",
    "\"Semen_Microbiology\",\n",
    "\"Blood_Microbiology\",\n",
    "\"Varicocele\",\n",
    "\"AMH (pmol/L)\",\t\n",
    "\"FSH (IU/L)\",\n",
    "\"LH (IU/L)\",\t\n",
    "\"Hormonal_Stimulation\",\n",
    "\"hCG_Trigger_Time (hours)\",\n",
    "\"Parity\",\n",
    "\"AFC (count)\",\n",
    "\"Previous_ICSI_Cycles\",\n",
    "\"Male_BMI (kg/m2)\",\t\n",
    "\"Female_BMI (kg/m2)\",\n",
    "\"Female_Tobacco\",\"Female_Alcohol\"])\n",
    "y = df[[\"Fertilization Rate (%)\",\"Blastulation_Rate (%)\"]]\n",
    "\n",
    "df.describe(); \n",
    "####Adding Feature Engineering \n",
    "# x[\"Motility_to_Count\"] = (\n",
    "#     df[\"Sperm_Prog_Motility (%)\"] / (df[\"Total_Sperm_Count (million)\"] + 1e-6)\n",
    "# )\n",
    "\n",
    "# x[\"Morph_to_Count\"] = (\n",
    "#     df[\"Sperm_Morphology (%)\"] / (df[\"Total_Sperm_Count (million)\"] + 1e-6)\n",
    "# )\n",
    "\n",
    "# print(x[[\"Motility_to_Count\", \"Morph_to_Count\"]].head())\n",
    "\n",
    "## In the below we encode our categorical columns \n",
    "le_col= {}\n",
    "for col in x.columns:\n",
    "    if x[col].dtype == 'object':\n",
    "        le = LabelEncoder()\n",
    "        x[col] = le.fit_transform(x[col].astype(str))\n",
    "        le_col[col] = le\n",
    "\n",
    "## Convert encoded columns to dataframes to inspect \n",
    "# Making sure all the data is correct and no nulls for input features \n",
    "encoded_df = x.copy()\n",
    "encoded_df.head()\n",
    "print(encoded_df.dtypes)\n",
    "print(encoded_df.describe())\n",
    "print(encoded_df.isnull().sum())\n",
    "\n",
    "## Making sure target variables are correct with no nulls \n",
    "print(y.head())          # sample values\n",
    "print(y.dtypes)          # ensure nsumeric\n",
    "print(y.nunique())\n",
    "print(y.isnull().sum())\n",
    "\n",
    "print(\"Remaining columns after deletion:\", x.columns.tolist())\n",
    "print(\"Total features:\", len(x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858fd8ce-836d-4eb6-879b-738d2fdd0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "skew_threshold =0.75\n",
    "\n",
    "skewness = df[numeric_cols].skew().sort_values(ascending=False)\n",
    "skewed_features = skewness[abs(skewness) > skew_threshold].index.tolist()\n",
    "\n",
    "print(f\"Highly skewed featers({len(skewed_features)}):\")\n",
    "print(skewed_features)\n",
    "print(\"Skewness per numeric column:\")\n",
    "print(skewness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc3a10-1365-43f0-b61b-08e626ea2665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Binning the target variables for classification\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "y_fert_binned = y[\"Fertilization Rate (%)\"].to_numpy() / 100 \n",
    "y_blast_binned = y[\"Blastulation_Rate (%)\"].to_numpy() / 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7de1d-ce9a-4ef8-a7aa-19abd2887dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING UP DATA \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "##Ensuring that the proportions of each class are preserved \n",
    "combined_labels = pd.Series(list(zip(y_fert_binned, y_blast_binned)) , name=\"combo\")\n",
    "\n",
    "##Checking for only vaalid combinations \n",
    "valid_combos= combined_labels.value_counts()[combined_labels.value_counts() >= 2].index\n",
    "mask = combined_labels.isin(valid_combos)\n",
    "x_filtered = encoded_df[mask]\n",
    "y_fert_filtered = y_fert_binned[mask]\n",
    "y_blast_filtered = y_blast_binned[mask]\n",
    "stratify_filtered = combined_labels[mask]\n",
    "\n",
    "# 1. FIRST SPLIT: Create temp and holdout sets\n",
    "x_temp, x_holdout, y_fert_temp, y_fert_holdout, y_blast_temp, y_blast_holdout = train_test_split(\n",
    "    x_filtered, y_fert_filtered, y_blast_filtered, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True,\n",
    "    # stratify=stratify_filtered  # Stratify on one target (both should have similar distributions)\n",
    ")\n",
    "\n",
    "##Ensuring that the proportions of each class are preserved \n",
    "stratify_temp = pd.Series(list(zip(y_fert_temp, y_blast_temp)))\n",
    "\n",
    "# 2. SECOND SPLIT: Create train and test sets from temp\n",
    "x_train, x_test, y_fert_train, y_fert_test, y_blast_train, y_blast_test = train_test_split(\n",
    "    x_temp, y_fert_temp, y_blast_temp,\n",
    "    test_size=0.3, \n",
    "    random_state=42, \n",
    "    shuffle=True,\n",
    "    # stratify=stratify_temp\n",
    ")\n",
    "\n",
    "##Ensuring that the proportions of each class are preserved \n",
    "stratify_train = pd.Series(list(zip(y_fert_train, y_blast_train)))\n",
    "\n",
    "# 3. THIRD SPLIT: Create train and validation sets from train\n",
    "x_train, x_val, y_fert_train, y_fert_val, y_blast_train, y_blast_val = train_test_split(\n",
    "    x_train, y_fert_train, y_blast_train,\n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    shuffle=True,\n",
    "    # stratify=stratify_train\n",
    ")\n",
    "\n",
    "print(\"Data shapes after splitting:\")\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"x_holdout shape:\", x_holdout.shape)\n",
    "\n",
    "#Checking the distribution of data\n",
    "print(\"\\nFertilization - Train distribution:\", np.bincount(y_fert_train.astype(int)))\n",
    "print(\"Fertilization - Validation distribution:\", np.bincount(y_fert_val.astype(int)))\n",
    "print(\"Fertilization - Test distribution:\", np.bincount(y_fert_test.astype(int)))\n",
    "print(\"Fertilization - Holdout distribution:\", np.bincount(y_fert_holdout.astype(int)))\n",
    "\n",
    "print(\"\\nBlastulation - Train distribution:\", np.bincount(y_blast_train.astype(int)))\n",
    "print(\"Blastulation - Validation distribution:\", np.bincount(y_blast_val.astype(int)))\n",
    "print(\"Blastulation - Test distribution:\", np.bincount(y_blast_test.astype(int)))\n",
    "print(\"Blastulation - Holdout distribution:\", np.bincount(y_blast_holdout.astype(int)))\n",
    "\n",
    "print(\"Fertilization:\", y_fert_train.min(), y_fert_train.max(), np.mean(y_fert_train))\n",
    "print(\"Blastulation:\", y_blast_train.min(), y_blast_train.max(), np.mean(y_blast_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720884d-4d9a-40fd-89c0-3e311ade516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting Panda series to numpy arrays\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "y_fert_train =np.array(y_fert_train , dtype=np.float32)\n",
    "y_fert_val   =np.array(y_fert_val , dtype=np.float32)\n",
    "y_blast_train = np.array(y_blast_train , dtype=np.float32)\n",
    "y_blast_val   = np.array(y_blast_val , dtype=np.float32)\n",
    "\n",
    "## Scaling my y-values \n",
    "epsilon = 1e-6\n",
    "y_fert_train = np.where(y_fert_train == 0, epsilon, y_fert_train)\n",
    "y_fert_train = np.where(y_fert_train == 1, 1 - epsilon, y_fert_train)\n",
    "\n",
    "y_fert_val = np.where(y_fert_val == 0, epsilon, y_fert_val)\n",
    "y_fert_val = np.where(y_fert_val == 1, 1 - epsilon, y_fert_val)\n",
    "\n",
    "y_blast_train = np.where(y_blast_train == 0, epsilon, y_blast_train)\n",
    "y_blast_train = np.where(y_blast_train == 1, 1 - epsilon, y_blast_train)\n",
    "\n",
    "y_blast_val = np.where(y_blast_val == 0, epsilon, y_blast_val)\n",
    "y_blast_val = np.where(y_blast_val == 1, 1 - epsilon, y_blast_val)\n",
    "\n",
    "print(\"Scaled Fertilization range:\", y_fert_train.min(), \"→\", y_fert_train.max())\n",
    "print(\"Scaled Blastulation range:\", y_blast_train.min(), \"→\", y_blast_train.max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cfb70-2d2d-4c4a-90ed-9c3326995f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##SCALING MY DATA \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import RobustScaler,StandardScaler\n",
    "\n",
    "## we will be capping the outliers within the 1st and the 99th percentiles \n",
    "def cap_outliers(x, lower=0.01, upper=0.99):\n",
    "    x_capped = x.copy().to_numpy()\n",
    "    for i in range(x.shape[1]):\n",
    "        low , high = np.quantile(x_capped[:,i] ,[lower, upper])\n",
    "        x_capped[:,i] = np.clip(x_capped[:,i], low, high)\n",
    "    return x_capped\n",
    "\n",
    "x_train_capped = cap_outliers(x_train)\n",
    "x_val_capped = cap_outliers(x_val)\n",
    "x_test_capped = cap_outliers(x_test)\n",
    "\n",
    "## Initialising standard Scaler and scaling our input features for model training\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_capped)\n",
    "x_test_scaled = scaler.transform(x_test_capped)\n",
    "x_val_scaled = scaler.transform(x_val_capped)\n",
    "x_holdout_scaled = scaler.transform(x_holdout)\n",
    "\n",
    "## Visualising data before and after Scaling \n",
    "x_train_scaled_df = pd.DataFrame(x_train_scaled, columns=x_train.columns)\n",
    "print(\"Before Scaling:\\n\", x_train.head())\n",
    "print(\"\\nAfter Scaling:\\n\", x_train_scaled_df.head())\n",
    "\n",
    "assert x_train_scaled.shape[0] == y_fert_train.shape[0] == y_blast_train.shape[0]\n",
    "assert x_val_scaled.shape[0] == y_fert_val.shape[0] == y_blast_val.shape[0]\n",
    "\n",
    "print(\"✓ All data shapes match!\")\n",
    "\n",
    "# Plotting 1 Feature to test Scaling\n",
    "feature = \"Sperm_Morphology (%)\"\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(x_train[feature] , bins=30)\n",
    "plt.title(f\"{feature} Before Scaling\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(x_train_scaled_df[feature], bins=30)\n",
    "plt.title(f\"{feature} After Scaling\")\n",
    "plt.show()\n",
    "\n",
    "print('x_train_scaled shape: ', x_train_scaled.shape)\n",
    "print('y_train_scaled shape: ', y_fert_train.shape)\n",
    "print('y_train_scaled shape: ', y_blast_train.shape)\n",
    "print(y_fert_train.shape, y_fert_train[:10], y_fert_train.dtype)\n",
    "print(y_blast_train.shape, y_blast_train[:10], y_blast_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1400cb6-b386-4cf1-8e80-0c8ff3039c87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf725d2-6bd3-401a-b4d7-3a8bb2d0fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Implementing beta negative likelihood (beta regression)\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def beta_nll_loss(y_true, y_pred):\n",
    "    mu = tf.clip_by_value(y_pred[:, 0], 1e-3, 1 - 1e-3)\n",
    "    phi = tf.nn.softplus(y_pred[:, 1]) + 1e-3\n",
    "    \n",
    "    alpha = mu * phi\n",
    "    beta = (1 - mu) * phi\n",
    "    \n",
    "    # Use log-beta function for stability\n",
    "    log_beta = tf.math.lgamma(alpha) + tf.math.lgamma(beta) - tf.math.lgamma(alpha + beta)\n",
    "    \n",
    "    # Log likelihood using safe operations\n",
    "    log_y = tf.math.log(tf.maximum(y_true, 1e-6))\n",
    "    log_1_minus_y = tf.math.log(tf.maximum(1 - y_true, 1e-6))\n",
    "    \n",
    "    log_likelihood = (alpha - 1) * log_y + (beta - 1) * log_1_minus_y\n",
    "    \n",
    "    nll = log_beta - log_likelihood\n",
    "    return tf.reduce_mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733724ec-7640-459d-9a52-28fea275f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "import math\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def evaluate_percent(y_true_scaled , p_pred_mu, name=\"model\"):\n",
    "# the y_true_scaled is in the 0-1 range we used in training as our target \n",
    "# y_pred_mu : the model predictions of(0-1 range)\n",
    "\n",
    "    y_true_pct = y_true_scaled *100.0\n",
    "    y_pred_pct = p_pred_mu  *100.0\n",
    "    mae = mean_absolute_error(y_true_pct, y_pred_pct)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true_pct, y_pred_pct))\n",
    "    print(f\"{name}  | MAE (pct): {mae:.3f}  | RMSE (pct): {rmse:.3f}\")\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f537fc-5ca2-4c6d-9575-1064ef2a0ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BUILD NEURAL NETWORK \n",
    "import tensorflow as tf \n",
    "from tensorflow import keras \n",
    "from tensorflow.keras import layers, Sequential, optimizers\n",
    "from tensorflow.keras.layers import BatchNormalization, LayerNormalization\n",
    "from tensorflow.keras.metrics import AUC\n",
    "from sklearn.metrics import classification_report, confusion_matrix , roc_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from tensorflow.keras.optimizers.schedules import CosineDecayRestarts\n",
    "\n",
    "\n",
    "#Clear any previous sessions \n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define OutputScaler layer\n",
    "class OutputScaler(layers.Layer):\n",
    "    def __init__(self, min_val, max_val, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mu = inputs[:, 0:1]\n",
    "        # Scale mu to match target distribution\n",
    "        mu_scaled = self.min_val + (self.max_val - self.min_val) * mu\n",
    "        phi = inputs[:, 1:2]  # Keep phi unchanged\n",
    "        return tf.concat([mu_scaled, phi], axis=1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"min_val\": self.min_val,\n",
    "            \"max_val\": self.max_val\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def mae_mu(y_true, y_pred):\n",
    "    mu = y_pred[:, 0]  \n",
    "    return tf.reduce_mean(tf.abs(y_true - mu))\n",
    "\n",
    "HP = {\n",
    "    \"lr\": 1e-3,                 # try 1e-3, 3e-4, 1e-4\n",
    "    \"weight_decay\": 1e-5, \n",
    "    \"l2_reg\": 1e-5,            # try 1e-6 .. 1e-4\n",
    "    \"units1\": 128,             # try 64, 128, 256\n",
    "    \"units2\": 64,              # try 32, 64\n",
    "    \"dropout\": 0.10,           # try 0.05, 0.1, 0.2\n",
    "    \"batch_size\": 16,          # try 16 or 32\n",
    "    \"patience_es\": 12,         # EarlyStopping patience\n",
    "    \"patience_rlr\": 6,         # ReduceLROnPlateau patience\n",
    "}\n",
    "\n",
    "noise_std = 0.01\n",
    "x_train_noisy = x_train_scaled + np.random.normal(0, noise_std, x_train_scaled.shape)\n",
    "def create_beta_model(input_dim, model_name, hp=HP, target_min=0.0, target_max=1.0):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(hp[\"units1\"], activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "                    kernel_regularizer=regularizers.l2(hp[\"l2_reg\"]))(inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = layers.Dropout(0.10)(x)\n",
    "\n",
    "    x = layers.Dense(hp[\"units2\"], activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "                    kernel_regularizer=regularizers.l2(hp[\"l2_reg\"]))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = layers.Dropout(0.10)(x)\n",
    "    \n",
    "    # Raw outputs (0-1 range)\n",
    "    mu_raw = layers.Dense(1, activation='sigmoid', name=f'{model_name}_mu_raw')(x)\n",
    "    phi = layers.Dense(1, activation='softplus', name=f'{model_name}_phi')(x)\n",
    "    \n",
    "    # Combine raw outputs\n",
    "    raw_outputs = layers.Concatenate(name=f'{model_name}_raw_output')([mu_raw, phi])\n",
    "    \n",
    "    outputs = OutputScaler(min_val=target_min, max_val=target_max, \n",
    "                          name=f'{model_name}_output')(raw_outputs)\n",
    "\n",
    "    # lr_schedule = CosineDecayRestarts(\n",
    "    #     initial_learning_rate = HP[\"lr\"], \n",
    "    #     first_decay_steps=80,\n",
    "    #     alpha=1e-4\n",
    "    # )\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=HP[\"lr\"] , weight_decay=HP[\"weight_decay\"])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=beta_nll_loss,\n",
    "        metrics=[mae_mu]\n",
    "    )\n",
    "    \n",
    "    return model \n",
    "\n",
    "input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "# Calculate actual ranges from your training data\n",
    "fert_min, fert_max = y_fert_train.min(), y_fert_train.max()\n",
    "blast_min, blast_max = y_blast_train.min(), y_blast_train.max()\n",
    "\n",
    "print(f\"Fertilization target range: {fert_min:.3f} - {fert_max:.3f}\")\n",
    "print(f\"Blastulation target range: {blast_min:.3f} - {blast_max:.3f}\")\n",
    "\n",
    "# Create models with appropriate scaling\n",
    "fert_model = create_beta_model(input_dim, \"fert\", target_min=fert_min, target_max=fert_max)\n",
    "blast_model = create_beta_model(input_dim, \"blast\", target_min=blast_min, target_max=blast_max)\n",
    "\n",
    "print(\"Fertilization Model Summary:\")\n",
    "fert_model.summary()\n",
    "print(\"\\nBlastulation Model Summary:\")\n",
    "blast_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8218dad7-0cb0-4962-b2dc-02eb5f6b5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# class WarmupCallback(tf.keras.callbacks.Callback):\n",
    "#     \"\"\"Gradual learning rate warmup for the first few epochs.\"\"\"\n",
    "#     def __init__(self, warmup_epochs, base_lr):\n",
    "#         super().__init__()\n",
    "#         self.warmup_epochs = warmup_epochs\n",
    "#         self.base_lr = float(base_lr)\n",
    "\n",
    "#     def on_epoch_begin(self, epoch, logs=None):\n",
    "#         opt = self.model.optimizer\n",
    "#         # Handle tf.Variable or callable LR\n",
    "#         if hasattr(opt, 'lr'):\n",
    "#             lr_attr = opt.lr\n",
    "#         elif hasattr(opt, 'learning_rate'):\n",
    "#             lr_attr = opt.learning_rate\n",
    "#         else:\n",
    "#             raise AttributeError(\"Optimizer has no lr or learning_rate attribute.\")\n",
    "\n",
    "#         if epoch < self.warmup_epochs:\n",
    "#             # Linear warmup\n",
    "#             lr = self.base_lr * (epoch + 1) / self.warmup_epochs\n",
    "#         else:\n",
    "#             lr = self.base_lr\n",
    "\n",
    "#         # Convert to float and set\n",
    "#         try:\n",
    "#             tf.keras.backend.set_value(lr_attr, lr)\n",
    "#         except Exception:\n",
    "#             # If it's not a tf.Variable, reassign it directly\n",
    "#             opt.learning_rate = tf.Variable(lr, dtype=tf.float32)\n",
    "\n",
    "#         print(f\"[Warmup] Epoch {epoch+1}/{self.warmup_epochs} - LR set to: {lr:.6e}\")\n",
    "##Implementing EarlyStopping and ModelCheckpoint\n",
    "# For us to save the best model and stop the model when it stops improving\n",
    "\n",
    "# fert_callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1),\n",
    "#     ModelCheckpoint('best_fert_model.keras', save_best_only=True, monitor='val_loss'),\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "#     # f1_callback_fert\n",
    "# ]\n",
    "\n",
    "# blast_callbacks = [\n",
    "#     EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True, verbose=1),\n",
    "#     ModelCheckpoint('best_blast_model.keras', save_best_only=True, monitor='val_loss'),\n",
    "#     ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6)\n",
    "#     # f1_callback_blast\n",
    "# ]\n",
    "\n",
    "# HP[\"lr\"] = float(HP[\"lr\"])\n",
    "\n",
    "# fert_callbacks.insert(0, WarmupCallback(warmup_epochs=5, base_lr=HP[\"lr\"]))\n",
    "# blast_callbacks.insert(0, WarmupCallback(warmup_epochs=5, base_lr=HP[\"lr\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16de5ff-ada2-44af-9964-d4637b0a9965",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import logit, expit\n",
    "\n",
    "y_fert_train_logit = logit(y_fert_train)\n",
    "y_fert_val_logit = logit(y_fert_val)\n",
    "y_blast_train_logit = logit(y_blast_train)\n",
    "y_blast_val_logit = logit(y_blast_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3624a834-04a9-4a39-a38a-0b40541c1d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing 5-Fold Cross Validation ---\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train_scaled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- Performing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-Fold Cross Validation ---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     14\u001b[39m fold = \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, val_idx \u001b[38;5;129;01min\u001b[39;00m kf.split(\u001b[43mx_train_scaled\u001b[49m):\n\u001b[32m     16\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m     \u001b[38;5;66;03m# Split data for this fold\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'x_train_scaled' is not defined"
     ]
    }
   ],
   "source": [
    "##Implementing K-fold cross validation \n",
    "from sklearn.model_selection import KFold \n",
    "import numpy as np \n",
    "\n",
    "# Number of folds (you can use 5 or 10 depending on dataset size)\n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fert_val_mae_scores = []\n",
    "blast_val_mae_scores = []\n",
    "\n",
    "print(f\"\\n--- Performing {k}-Fold Cross Validation ---\\n\")\n",
    "\n",
    "fold = 1\n",
    "for train_idx, val_idx in kf.split(x_train_scaled):\n",
    "    print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "    # Split data for this fold\n",
    "    X_train_fold, X_val_fold = x_train_scaled[train_idx], x_train_scaled[val_idx]\n",
    "    y_fert_train_fold, y_fert_val_fold = y_fert_train[train_idx], y_fert_train[val_idx]\n",
    "    y_blast_train_fold, y_blast_val_fold = y_blast_train[train_idx], y_blast_train[val_idx]\n",
    "\n",
    "    # Create fresh models for this fold\n",
    "    fert_model = create_beta_model(input_dim, \"fert\", target_min=fert_min, target_max=fert_max)\n",
    "    blast_model = create_beta_model(input_dim, \"blast\", target_min=blast_min, target_max=blast_max)\n",
    "\n",
    "    # Define callbacks\n",
    "    fert_callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=HP[\"patience_es\"], restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', patience=HP[\"patience_rlr\"], factor=0.5, min_lr=1e-6)\n",
    "    ]\n",
    "    blast_callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=HP[\"patience_es\"], restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', patience=HP[\"patience_rlr\"], factor=0.5, min_lr=1e-6)\n",
    "    ]\n",
    "\n",
    "    # --- Train Fertilization Model ---\n",
    "    fert_model.fit(\n",
    "        X_train_fold, y_fert_train_fold,\n",
    "        validation_data=(X_val_fold, y_fert_val_fold),\n",
    "        epochs=100,\n",
    "        batch_size=HP[\"batch_size\"],\n",
    "        callbacks=fert_callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # --- Train Blastulation Model ---\n",
    "    blast_model.fit(\n",
    "        X_train_fold, y_blast_train_fold,\n",
    "        validation_data=(X_val_fold, y_blast_val_fold),\n",
    "        epochs=100,\n",
    "        batch_size=HP[\"batch_size\"],\n",
    "        callbacks=blast_callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # --- Evaluate each fold ---\n",
    "    fert_eval = fert_model.evaluate(X_val_fold, y_fert_val_fold, verbose=0)\n",
    "    blast_eval = blast_model.evaluate(X_val_fold, y_blast_val_fold, verbose=0)\n",
    "\n",
    "    fert_val_mae_scores.append(fert_eval[1])  # mae_mu is index 1\n",
    "    blast_val_mae_scores.append(blast_eval[1])\n",
    "\n",
    "    print(f\"  Fert MAE_mu: {fert_eval[1]:.4f} | Blast MAE_mu: {blast_eval[1]:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# --- Summarize Cross-Validation Results ---\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Fertilization MAE_mu (mean ± std): {np.mean(fert_val_mae_scores):.4f} ± {np.std(fert_val_mae_scores):.4f}\")\n",
    "print(f\"Blastulation MAE_mu (mean ± std):   {np.mean(blast_val_mae_scores):.4f} ± {np.std(blast_val_mae_scores):.4f}\")\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "# --- Retrain final models on full training data ---\n",
    "print(\"\\nRetraining final models on full training data...\")\n",
    "\n",
    "fert_model = create_beta_model(input_dim, \"fert\", target_min=fert_min, target_max=fert_max)\n",
    "blast_model = create_beta_model(input_dim, \"blast\", target_min=blast_min, target_max=blast_max)\n",
    "\n",
    "fert_model.fit(\n",
    "    x_train_scaled, y_fert_train,\n",
    "    validation_data=(x_val_scaled, y_fert_val),\n",
    "    epochs=100,\n",
    "    batch_size=HP[\"batch_size\"],\n",
    "    callbacks=fert_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "blast_model.fit(\n",
    "    x_train_scaled, y_blast_train,\n",
    "    validation_data=(x_val_scaled, y_blast_val),\n",
    "    epochs=100,\n",
    "    batch_size=HP[\"batch_size\"],\n",
    "    callbacks=blast_callbacks,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285e16a-f685-4df7-8367-74b60e2fd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scale model outputs to realistic 0-100% range ---\n",
    "\n",
    "# Option 1: fixed range\n",
    "y_min_fert, y_max_fert = 0, 100\n",
    "y_min_blast, y_max_blast = 0, 100\n",
    "\n",
    "# 1. Get mu predictions from models\n",
    "y_pred_mu_fert = expit(fert_model.predict(x_val_scaled)[:, 0])\n",
    "evaluate_percent(y_fert_val, y_pred_mu_fert, name=\"Fert (val)\")\n",
    "\n",
    "y_pred_mu_blast = expit(blast_model.predict(x_val_scaled)[:, 0])\n",
    "evaluate_percent(y_blast_val, y_pred_mu_blast, name=\"blast (val)\")\n",
    "\n",
    "# 2. Scale to percentage\n",
    "y_pred_percent_fert = y_min_fert + (y_max_fert - y_min_fert) * y_pred_mu_fert\n",
    "y_pred_percent_blast = y_min_blast + (y_max_blast - y_min_blast) * y_pred_mu_blast\n",
    "\n",
    "# 3. Check distributions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fertilization\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y_pred_mu_fert, bins=20)\n",
    "plt.title(\"Predicted μ (fert) 0-1 scale\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_pred_percent_fert, bins=20)\n",
    "plt.title(\"Predicted Fertilization %\")\n",
    "plt.show()\n",
    "\n",
    "# Blastulation\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y_pred_mu_blast, bins=20)\n",
    "plt.title(\"Predicted μ (blast) 0-1 scale\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_pred_percent_blast, bins=20)\n",
    "plt.title(\"Predicted Blastulation %\")\n",
    "plt.show()\n",
    "\n",
    "# Print summary stats\n",
    "print(\"Fertilization (%) - min, max, mean:\", y_pred_percent_fert.min(), y_pred_percent_fert.max(), np.mean(y_pred_percent_fert))\n",
    "print(\"Blastulation (%) - min, max, mean:\", y_pred_percent_blast.min(), y_pred_percent_blast.max(), np.mean(y_pred_percent_blast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b8656-b8b8-4514-b214-5acc2bb76f79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Sample y_fert_train (unscaled):\", y_fert_train[:10])\n",
    "print(\"Sample y_fert_train * 100:\", (y_fert_train * 100)[:10])\n",
    "# Convert back to percentage scale\n",
    "y_pred_fert_pct = y_pred_mu_fert * 100\n",
    "y_pred_blast_pct = y_pred_mu_blast * 100\n",
    "\n",
    "# Same for true labels\n",
    "y_true_fert_pct = y_fert_val * 100\n",
    "y_true_blast_pct = y_blast_val * 100\n",
    "\n",
    "print(\"Fertilization True (%) - min, max, mean:\", \n",
    "      np.min(y_true_fert_pct), np.max(y_true_fert_pct), np.mean(y_true_fert_pct))\n",
    "print(\"Fertilization Pred (%) - min, max, mean:\", \n",
    "      np.min(y_pred_fert_pct), np.max(y_pred_fert_pct), np.mean(y_pred_fert_pct))\n",
    "\n",
    "print(\"Blastulation True (%) - min, max, mean:\", \n",
    "      np.min(y_true_blast_pct), np.max(y_true_blast_pct), np.mean(y_true_blast_pct))\n",
    "print(\"Blastulation Pred (%) - min, max, mean:\", \n",
    "      np.min(y_pred_blast_pct), np.max(y_pred_blast_pct), np.mean(y_pred_blast_pct))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1737b-4d77-4353-8031-0d0fb4dc8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "print(\"\\n--- PHASE 5: Extended Evaluation ---\")\n",
    "\n",
    "# Evaluate on TEST set\n",
    "print(\"\\nTesting on unseen test set...\")\n",
    "fert_test_pred = expit(fert_model.predict(x_test_scaled)[:, 0])\n",
    "blast_test_pred = expit(blast_model.predict(x_test_scaled)[:, 0])\n",
    "\n",
    "evaluate_percent(y_fert_test, fert_test_pred, name=\"Fert (test)\")\n",
    "evaluate_percent(y_blast_test, blast_test_pred, name=\"Blast (test)\")\n",
    "\n",
    "# Evaluate on HOLDOUT set (completely unseen)\n",
    "print(\"\\nEvaluating on final holdout set...\")\n",
    "fert_holdout_pred = expit(fert_model.predict(x_holdout_scaled)[:, 0])\n",
    "blast_holdout_pred = expit(blast_model.predict(x_holdout_scaled)[:, 0])\n",
    "\n",
    "evaluate_percent(y_fert_holdout, fert_holdout_pred, name=\"Fert (holdout)\")\n",
    "evaluate_percent(y_blast_holdout, blast_holdout_pred, name=\"Blast (holdout)\")\n",
    "\n",
    "\n",
    "# Comparison plots for validation vs. predictions\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(y_fert_val * 100, fert_model.predict(x_val_scaled)[:,0]*100, alpha=0.6)\n",
    "plt.xlabel(\"True Fertilization (%)\")\n",
    "plt.ylabel(\"Predicted Fertilization (%)\")\n",
    "plt.title(\"Validation Fertilization\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_blast_val * 100, blast_model.predict(x_val_scaled)[:,0]*100, alpha=0.6)\n",
    "plt.xlabel(\"True Blastulation (%)\")\n",
    "plt.ylabel(\"Predicted Blastulation (%)\")\n",
    "plt.title(\"Validation Blastulation\")\n",
    "plt.show()\n",
    "\n",
    "# Save trained models\n",
    "fert_model.save(\"final_fertilization_model.keras\")\n",
    "blast_model.save(\"final_blastulation_model.keras\")\n",
    "\n",
    "# Save scaler for future inference\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "print(\"✓ Models and scaler successfully saved for deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70703c7-0500-4cc1-9666-9a90451a0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rates(new_data_df):\n",
    "    \"\"\"\n",
    "    Input: new_data_df (pandas DataFrame with same columns as x_train)\n",
    "    Output: dict with Fertilization% and Blastulation%\n",
    "    \"\"\"\n",
    "    # Load artifacts if needed\n",
    "    from tensorflow.keras.models import load_model\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    from scipy.special import expit\n",
    "\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    fert_model = load_model(\"final_fertilization_model.keras\", custom_objects={\"beta_nll_loss\": beta_nll_loss, \"mae_mu\": mae_mu, \"OutputScaler\": OutputScaler})\n",
    "    blast_model = load_model(\"final_blastulation_model.keras\", custom_objects={\"beta_nll_loss\": beta_nll_loss, \"mae_mu\": mae_mu, \"OutputScaler\": OutputScaler})\n",
    "\n",
    "    # Scale new inputs\n",
    "    new_scaled = scaler.transform(new_data_df)\n",
    "\n",
    "    # Predict\n",
    "    fert_pred = expit(fert_model.predict(new_scaled)[:,0]) * 100\n",
    "    blast_pred = expit(blast_model.predict(new_scaled)[:,0]) * 100\n",
    "\n",
    "    return {\n",
    "        \"Fertilization_Rate(%)\": np.mean(fert_pred),\n",
    "        \"Blastulation_Rate(%)\": np.mean(blast_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f099d52f-63a5-41e3-baf2-9289a9d468bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
