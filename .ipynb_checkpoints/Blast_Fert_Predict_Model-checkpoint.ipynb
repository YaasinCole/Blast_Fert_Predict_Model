{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59aa0fa6-cc17-418e-9fe5-e43aa5f225e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LayerNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f01e012-23c4-499a-9aa8-cbf0a740c398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'RBL Data Edited - Combined.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m## DATA PREPROCESSING\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRBL Data Edited - Combined.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m;\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiprows\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecimal\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m,\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m \u001b[49m\u001b[43m!=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPatient_ID\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m## Dropping null values from the dataset\u001b[39;00m\n\u001b[32m      4\u001b[39m df = df.dropna(how=\u001b[33m\"\u001b[39m\u001b[33mall\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'RBL Data Edited - Combined.csv'"
     ]
    }
   ],
   "source": [
    "## DATA PREPROCESSING\n",
    "df = pd.read_csv('RBL Data Edited-Combined.csv', sep=\";\" , skiprows=1 , decimal=\",\", encoding=\"utf-8\" , usecols=lambda col: col != 'Patient_ID')\n",
    "## Dropping null values from the dataset\n",
    "df = df.dropna(how=\"all\")\n",
    "df.isnull().sum()\n",
    "\n",
    "#after the above we then separate the targets from input features also dropping unimportant features from the set\n",
    "x = df.drop(columns=[\"Fertilization Rate (%)\" , \"Blastulation_Rate (%)\",\"Year\"])\n",
    "y = df[[\"Fertilization Rate (%)\",\"Blastulation_Rate (%)\"]]\n",
    "\n",
    "## Added step for dealing with missing values \n",
    "num_cols= x.select_dtypes(include=[\"float64\", \"int64\"]).columns\n",
    "cat_cols= x.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "num_imputer= SimpleImputer(strategy=\"median\")\n",
    "cat_imputer= SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "x[num_cols]= num_imputer.fit_transform(x[num_cols])\n",
    "x[cat_cols]= cat_imputer.fit_transform(x[cat_cols])\n",
    "\n",
    "## In the below we encode our categorical columns \n",
    "le_col= {}\n",
    "for col in cat_cols:\n",
    "        le = LabelEncoder()\n",
    "        x[col] = le.fit_transform(x[col].astype(str))\n",
    "        le_col[col] = le\n",
    "\n",
    "## Convert encoded columns to dataframes to inspect \n",
    "# Making sure all the data is correct and no nulls for input features \n",
    "encoded_df = x.copy()\n",
    "encoded_df.head()\n",
    "print(encoded_df.dtypes)\n",
    "print(encoded_df.describe())\n",
    "print(encoded_df.isnull().sum())\n",
    "\n",
    "## Making sure target variables are correct with no nulls \n",
    "print(y.head())          # sample values\n",
    "print(y.dtypes)          # ensure nsumeric\n",
    "print(y.nunique())\n",
    "print(y.isnull().sum())\n",
    "\n",
    "print(df.shape)\n",
    "print(\"Total features:\", len(x.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabc3a10-1365-43f0-b61b-08e626ea2665",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale targets to (0,1) for beta regression\n",
    "y_fert_binned = y[\"Fertilization Rate (%)\"].str.replace(\"%\",\"\", regex=False).astype(float) / 100.0 \n",
    "y_blast_binned = y[\"Blastulation_Rate (%)\"].str.replace(\"%\",\"\", regex=False).astype(float) / 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f7de1d-ce9a-4ef8-a7aa-19abd2887dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# DATA SPLITTING\n",
    "# =========================\n",
    "\n",
    "# Use FULL dataset (no filtering)\n",
    "X = encoded_df\n",
    "y_fert = y_fert_binned\n",
    "y_blast = y_blast_binned\n",
    "\n",
    "# 1. FIRST SPLIT: Temp + Holdout (20% holdout)\n",
    "x_temp, x_holdout, y_fert_temp, y_fert_holdout, y_blast_temp, y_blast_holdout = train_test_split(\n",
    "    X,\n",
    "    y_fert,\n",
    "    y_blast,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 2. SECOND SPLIT: Train + Test (30% of temp → test)\n",
    "x_train, x_test, y_fert_train, y_fert_test, y_blast_train, y_blast_test = train_test_split(\n",
    "    x_temp,\n",
    "    y_fert_temp,\n",
    "    y_blast_temp,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# 3. THIRD SPLIT: Train + Validation (20% of train → val)\n",
    "x_train, x_val, y_fert_train, y_fert_val, y_blast_train, y_blast_val = train_test_split(\n",
    "    x_train,\n",
    "    y_fert_train,\n",
    "    y_blast_train,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# =========================\n",
    "# SANITY CHECKS\n",
    "# =========================\n",
    "print(\"Data shapes after splitting:\")\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(\"x_val shape:\", x_val.shape)\n",
    "print(\"x_test shape:\", x_test.shape)\n",
    "print(\"x_holdout shape:\", x_holdout.shape)\n",
    "\n",
    "print(\"Fertilization (train):\",\n",
    "      y_fert_train.min(), y_fert_train.max(), np.mean(y_fert_train))\n",
    "print(\"Blastulation (train):\",\n",
    "      y_blast_train.min(), y_blast_train.max(), np.mean(y_blast_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0720884d-4d9a-40fd-89c0-3e311ade516b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Converting Panda series to numpy arrays\n",
    "y_fert_train =np.array(y_fert_train , dtype=np.float32)\n",
    "y_fert_val   =np.array(y_fert_val , dtype=np.float32)\n",
    "y_blast_train = np.array(y_blast_train , dtype=np.float32)\n",
    "y_blast_val   = np.array(y_blast_val , dtype=np.float32)\n",
    "\n",
    "## Scaling my y-values \n",
    "epsilon = 1e-6\n",
    "y_fert_train = np.clip(y_fert_train, epsilon, 1 - epsilon)\n",
    "y_fert_val = np.clip(y_fert_val, epsilon, 1 - epsilon)\n",
    "\n",
    "y_blast_train = np.clip(y_blast_train, epsilon, 1 - epsilon)\n",
    "y_blast_val = np.clip(y_blast_val, epsilon, 1 - epsilon)\n",
    "\n",
    "print(\"Scaled Fertilization range:\", y_fert_train.min(), \"→\", y_fert_train.max())\n",
    "print(\"Scaled Blastulation range:\", y_blast_train.min(), \"→\", y_blast_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cfb70-2d2d-4c4a-90ed-9c3326995f76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##SCALING MY DATA \n",
    "## we will be capping the outliers within the 1st and the 99th percentiles \n",
    "def fit_outlier_caps(x, lower=0.01, upper=0.99):\n",
    "    bounds = []\n",
    "    x_np = x.to_numpy()\n",
    "    for i in range(x_np.shape[1]):\n",
    "        lo, hi = np.quantile(x_np[:, i], [lower, upper])\n",
    "        bounds.append((lo, hi))\n",
    "    return bounds\n",
    "\n",
    "def apply_outlier_caps(x, bounds):\n",
    "    x_np = x.to_numpy()\n",
    "    for i, (lo, hi) in enumerate(bounds):\n",
    "        x_np[:, i] = np.clip(x_np[:, i], lo, hi)\n",
    "    return x_np\n",
    "\n",
    "\n",
    "outlier_bounds = fit_outlier_caps(x_train)\n",
    "\n",
    "x_train_capped = apply_outlier_caps(x_train, outlier_bounds)\n",
    "x_val_capped = apply_outlier_caps(x_val, outlier_bounds)\n",
    "x_test_capped = apply_outlier_caps(x_test, outlier_bounds)\n",
    "x_holdout_capped = apply_outlier_caps(x_holdout, outlier_bounds)\n",
    "## Initialising standard Scaler and scaling our input features for model training\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train_capped)\n",
    "x_test_scaled = scaler.transform(x_test_capped)\n",
    "x_val_scaled = scaler.transform(x_val_capped)\n",
    "x_holdout_scaled = scaler.transform(x_holdout_capped)\n",
    "\n",
    "## Visualising data before and after Scaling \n",
    "x_train_scaled_df = pd.DataFrame(x_train_scaled, columns=x_train.columns)\n",
    "print(\"Before Scaling:\\n\", x_train.head())\n",
    "print(\"\\nAfter Scaling:\\n\", x_train_scaled_df.head())\n",
    "\n",
    "assert x_train_scaled.shape[0] == y_fert_train.shape[0] == y_blast_train.shape[0]\n",
    "assert x_val_scaled.shape[0] == y_fert_val.shape[0] == y_blast_val.shape[0]\n",
    "\n",
    "print(\"All data shapes match!\")\n",
    "print('x_train_scaled shape: ', x_train_scaled.shape)\n",
    "print('y_train_scaled shape: ', y_fert_train.shape)\n",
    "print('y_train_scaled shape: ', y_blast_train.shape)\n",
    "print(y_fert_train.shape, y_fert_train[:10], y_fert_train.dtype)\n",
    "print(y_blast_train.shape, y_blast_train[:10], y_blast_train.dtype)\n",
    "\n",
    "print(\"Final Fert range:\", y_fert_train.min(), y_fert_train.max())\n",
    "print(\"Final Blast range:\", y_blast_train.min(), y_blast_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf725d2-6bd3-401a-b4d7-3a8bb2d0fc88",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Implementing beta negative likelihood (beta regression)\n",
    "def beta_nll_loss(y_true, y_pred):\n",
    "    mu = tf.clip_by_value(y_pred[:, 0], 1e-3, 1 - 1e-3)\n",
    "    phi = tf.nn.softplus(y_pred[:, 1]) + 1e-3\n",
    "    \n",
    "    alpha = mu * phi\n",
    "    beta = (1 - mu) * phi\n",
    "    \n",
    "    # Use log-beta function for stability\n",
    "    log_beta = tf.math.lgamma(alpha) + tf.math.lgamma(beta) - tf.math.lgamma(alpha + beta)\n",
    "    \n",
    "    # Log likelihood using safe operations\n",
    "    log_y = tf.math.log(tf.maximum(y_true, 1e-6))\n",
    "    log_1_minus_y = tf.math.log(tf.maximum(1 - y_true, 1e-6))\n",
    "    \n",
    "    log_likelihood = (alpha - 1) * log_y + (beta - 1) * log_1_minus_y\n",
    "    \n",
    "    nll = log_beta - log_likelihood\n",
    "    return tf.reduce_mean(nll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733724ec-7640-459d-9a52-28fea275f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_percent(y_true_scaled , p_pred_mu, name=\"model\"):\n",
    "# the y_true_scaled is in the 0-1 range we used in training as our target \n",
    "# y_pred_mu : the model predictions of(0-1 range)\n",
    "\n",
    "    y_true_pct = y_true_scaled *100.0\n",
    "    y_pred_pct = p_pred_mu  *100.0\n",
    "    mae = mean_absolute_error(y_true_pct, y_pred_pct)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true_pct, y_pred_pct))\n",
    "    print(f\"{name}  | MAE (pct): {mae:.3f}  | RMSE (pct): {rmse:.3f}\")\n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f537fc-5ca2-4c6d-9575-1064ef2a0ee0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#BUILD NEURAL NETWORK \n",
    "#Clear any previous sessions \n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Define OutputScaler layer\n",
    "class OutputScaler(layers.Layer):\n",
    "    def __init__(self, min_val, max_val, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.min_val = min_val\n",
    "        self.max_val = max_val\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mu = inputs[:, 0:1]\n",
    "        # Scale mu to match target distribution\n",
    "        mu_scaled = self.min_val + (self.max_val - self.min_val) * mu\n",
    "        phi = inputs[:, 1:2]  # Keep phi unchanged\n",
    "        return tf.concat([mu_scaled, phi], axis=1)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"min_val\": self.min_val,\n",
    "            \"max_val\": self.max_val\n",
    "        })\n",
    "        return config\n",
    "\n",
    "def mae_mu(y_true, y_pred):\n",
    "    mu = y_pred[:, 0]  \n",
    "    return tf.reduce_mean(tf.abs(y_true - mu))\n",
    "\n",
    "HP = {\n",
    "    \"lr\": 1e-3,                \n",
    "    \"weight_decay\": 1e-5, \n",
    "    \"l2_reg\": 1e-5,            \n",
    "    \"units1\": 128,             \n",
    "    \"units2\": 64,            \n",
    "    \"dropout\": 0.10,          \n",
    "    \"batch_size\": 16,          \n",
    "    \"patience_es\": 12,        \n",
    "    \"patience_rlr\": 6,         \n",
    "}\n",
    "\n",
    "def create_beta_model(input_dim, model_name, hp=HP, target_min=0.0, target_max=1.0):\n",
    "    inputs = layers.Input(shape=(input_dim,))\n",
    "    x = layers.Dense(hp[\"units1\"], activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "                    kernel_regularizer=regularizers.l2(hp[\"l2_reg\"]))(inputs)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = layers.Dropout(0.10)(x)\n",
    "\n",
    "    x = layers.Dense(hp[\"units2\"], activation=tf.keras.layers.LeakyReLU(alpha=0.1),\n",
    "                    kernel_regularizer=regularizers.l2(hp[\"l2_reg\"]))(x)\n",
    "    x = LayerNormalization()(x)\n",
    "    x = layers.Dropout(0.10)(x)\n",
    "    \n",
    "    # Raw outputs (0-1 range)\n",
    "    mu_raw = layers.Dense(1, activation='sigmoid', name=f'{model_name}_mu_raw')(x)\n",
    "    phi = layers.Dense(1, activation='softplus', name=f'{model_name}_phi')(x)\n",
    "    \n",
    "    # Combine raw outputs\n",
    "    raw_outputs = layers.Concatenate(name=f'{model_name}_raw_output')([mu_raw, phi])\n",
    "    \n",
    "    outputs = OutputScaler(min_val=target_min, max_val=target_max, \n",
    "                          name=f'{model_name}_output')(raw_outputs)\n",
    "\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    optimizer = tf.keras.optimizers.AdamW(learning_rate=HP[\"lr\"] , weight_decay=HP[\"weight_decay\"])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=beta_nll_loss,\n",
    "        metrics=[mae_mu]\n",
    "    )\n",
    "    \n",
    "    return model \n",
    "\n",
    "input_dim = x_train_scaled.shape[1]\n",
    "\n",
    "# Calculate actual ranges from your training data\n",
    "fert_min, fert_max = y_fert_train.min(), y_fert_train.max()\n",
    "blast_min, blast_max = y_blast_train.min(), y_blast_train.max()\n",
    "\n",
    "print(f\"Fertilization target range: {fert_min:.3f} - {fert_max:.3f}\")\n",
    "print(f\"Blastulation target range: {blast_min:.3f} - {blast_max:.3f}\")\n",
    "\n",
    "# Create models with appropriate scaling\n",
    "fert_model = create_beta_model(input_dim, \"fert\", target_min=fert_min, target_max=fert_max)\n",
    "blast_model = create_beta_model(input_dim, \"blast\", target_min=blast_min, target_max=blast_max)\n",
    "\n",
    "print(\"Fertilization Model Summary:\")\n",
    "fert_model.summary()\n",
    "print(\"\\nBlastulation Model Summary:\")\n",
    "blast_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3624a834-04a9-4a39-a38a-0b40541c1d4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##Implementing K-fold cross validation \n",
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "fert_val_mae_scores = []\n",
    "\n",
    "print(f\"\\n--- Performing {k}-Fold Cross Validation (Fertilisation Model) ---\\n\")\n",
    "\n",
    "fold = 1\n",
    "for train_idx, val_idx in kf.split(x_train_scaled):\n",
    "    print(f\"Fold {fold}/{k}\")\n",
    "\n",
    "    # Split data for this fold\n",
    "    X_train_fold, X_val_fold = x_train_scaled[train_idx], x_train_scaled[val_idx]\n",
    "    y_fert_train_fold, y_fert_val_fold = y_fert_train[train_idx], y_fert_train[val_idx]\n",
    "\n",
    "    # Create new model for each fold\n",
    "    fert_model = create_beta_model(input_dim, \"fert\", target_min=fert_min, target_max=fert_max)\n",
    "\n",
    "    fert_callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=HP[\"patience_es\"], restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', patience=HP[\"patience_rlr\"], factor=0.5, min_lr=1e-6)\n",
    "    ]\n",
    "\n",
    "    fert_model.fit(\n",
    "        X_train_fold, y_fert_train_fold,\n",
    "        validation_data=(X_val_fold, y_fert_val_fold),\n",
    "        epochs=100,\n",
    "        batch_size=HP[\"batch_size\"],\n",
    "        callbacks=fert_callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    fert_eval = fert_model.evaluate(X_val_fold, y_fert_val_fold, verbose=0)\n",
    "    fert_val_mae_scores.append(fert_eval[1])  # mae_mu index\n",
    "\n",
    "    print(f\"  Fold {fold} Fert MAE_mu: {fert_eval[1]:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "# --- Summarize Cross-Validation Results ---\n",
    "# Cross-validation performed only on fertilisation model\n",
    "# to assess base predictive stability before conditional modelling\n",
    "print(\"\\nCross-Validation Results:\")\n",
    "print(f\"Fertilisation MAE_mu (mean ± std): {np.mean(fert_val_mae_scores):.4f} ± {np.std(fert_val_mae_scores):.4f}\")\n",
    "print(\"------------------------------------------------------\")\n",
    "\n",
    "\n",
    "# --- Retrain final fertilisation model on full training data ---\n",
    "print(\"\\nRetraining fertilisation model on full training data...\")\n",
    "\n",
    "fert_model = create_beta_model(input_dim, \"fert\", target_min=fert_min, target_max=fert_max)\n",
    "fert_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=HP[\"patience_es\"], restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', patience=HP[\"patience_rlr\"], factor=0.5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "fert_model.fit(\n",
    "    x_train_scaled, y_fert_train,\n",
    "    validation_data=(x_val_scaled, y_fert_val),\n",
    "    epochs=100,\n",
    "    batch_size=HP[\"batch_size\"],\n",
    "    callbacks=fert_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# --- Combine training and validation sets once ---\n",
    "x_full_scaled = np.concatenate([x_train_scaled, x_val_scaled], axis=0)\n",
    "y_blast_full = np.concatenate([y_blast_train, y_blast_val], axis=0)\n",
    "\n",
    "# --- Predict fertilisation rate for exactly the same rows ---\n",
    "fert_pred_full = fert_model.predict(x_full_scaled)[:, 0]\n",
    "print(f\"x_full_scaled: {x_full_scaled.shape}, fert_pred_full: {fert_pred_full.shape}\")\n",
    "\n",
    "# --- Filter for non-zero fertilisation ---\n",
    "valid_idx = fert_pred_full > 0\n",
    "\n",
    "# --- Apply same mask across all arrays ---\n",
    "x_blast = x_full_scaled[valid_idx]\n",
    "y_blast = y_blast_full[valid_idx]\n",
    "fert_pred_valid = fert_pred_full[valid_idx]\n",
    "\n",
    "# --- Prepare data for blastulation model --\n",
    "\n",
    "# Enrich with predicted fertilisation as an input\n",
    "x_blast_enriched = np.concatenate([x_blast, fert_pred_valid.reshape(-1, 1)], axis=1)\n",
    "\n",
    "print(f\"\\nTraining Blastulation Model on {x_blast_enriched.shape[0]} samples (fert > 0)\")\n",
    "\n",
    "# --- Train Blastulation Model ---\n",
    "blast_model = create_beta_model(\n",
    "    input_dim=x_blast_enriched.shape[1],\n",
    "    model_name=\"blast\",\n",
    "    target_min=blast_min,\n",
    "    target_max=blast_max\n",
    ")\n",
    "\n",
    "blast_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=HP[\"patience_es\"], restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', patience=HP[\"patience_rlr\"], factor=0.5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "blast_model.fit(\n",
    "    x_blast_enriched, y_blast,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=HP[\"batch_size\"],\n",
    "    callbacks=blast_callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✅ Both models trained successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285e16a-f685-4df7-8367-74b60e2fd3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Scale model outputs to realistic 0-100% range ---\n",
    "# Option 1: fixed range\n",
    "y_min_fert, y_max_fert = 0, 100\n",
    "y_min_blast, y_max_blast = 0, 100\n",
    "\n",
    "# 1. Get mu predictions from models\n",
    "y_pred_mu_fert = fert_model.predict(x_val_scaled)[:, 0]\n",
    "evaluate_percent(y_fert_val, y_pred_mu_fert, name=\"Fert (val)\")\n",
    "\n",
    "x_val_blast_enriched = np.concatenate([x_val_scaled , y_pred_mu_fert.reshape(-1,1)], \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "y_pred_mu_blast = blast_model.predict(x_val_blast_enriched)[:, 0]\n",
    "evaluate_percent(y_blast_val, y_pred_mu_blast, name=\"blast (val)\")\n",
    "\n",
    "# 2. Scale to percentage\n",
    "y_pred_percent_fert = y_min_fert + (y_max_fert - y_min_fert) * y_pred_mu_fert\n",
    "y_pred_percent_blast = y_min_blast + (y_max_blast - y_min_blast) * y_pred_mu_blast\n",
    "\n",
    "# Fertilization\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y_pred_mu_fert, bins=20)\n",
    "plt.title(\"Predicted μ (fert) 0-1 scale\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_pred_percent_fert, bins=20)\n",
    "plt.title(\"Predicted Fertilization %\")\n",
    "plt.show()\n",
    "\n",
    "# Blastulation\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(y_pred_mu_blast, bins=20)\n",
    "plt.title(\"Predicted μ (blast) 0-1 scale\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(y_pred_percent_blast, bins=20)\n",
    "plt.title(\"Predicted Blastulation %\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample y_fert_train (unscaled):\", y_fert_train[:10])\n",
    "print(\"Sample y_fert_train * 100:\", (y_fert_train * 100)[:10])\n",
    "# Convert back to percentage scale\n",
    "y_pred_fert_pct = y_pred_mu_fert * 100\n",
    "y_pred_blast_pct = y_pred_mu_blast * 100\n",
    "\n",
    "# Same for true labels\n",
    "y_true_fert_pct = y_fert_val * 100\n",
    "y_true_blast_pct = y_blast_val * 100\n",
    "\n",
    "print(\"Fertilization True (%) - min, max, mean:\", \n",
    "      np.min(y_true_fert_pct), np.max(y_true_fert_pct), np.mean(y_true_fert_pct))\n",
    "print(\"Fertilization Pred (%) - min, max, mean:\", \n",
    "      np.min(y_pred_fert_pct), np.max(y_pred_fert_pct), np.mean(y_pred_fert_pct))\n",
    "\n",
    "print(\"Blastulation True (%) - min, max, mean:\", \n",
    "      np.min(y_true_blast_pct), np.max(y_true_blast_pct), np.mean(y_true_blast_pct))\n",
    "print(\"Blastulation Pred (%) - min, max, mean:\", \n",
    "      np.min(y_pred_blast_pct), np.max(y_pred_blast_pct), np.mean(y_pred_blast_pct))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b1737b-4d77-4353-8031-0d0fb4dc8b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- PHASE 5: Extended Evaluation ---\")\n",
    "\n",
    "# Evaluate on TEST set\n",
    "print(\"\\nTesting on unseen test set...\")\n",
    "fert_test_pred = fert_model.predict(x_test_scaled)[:, 0]\n",
    "evaluate_percent(y_fert_test, fert_test_pred, name=\"Fert (test)\")\n",
    "\n",
    "x_test_blast_enriched = np.concatenate([x_test_scaled , fert_test_pred.reshape(-1,1)],\n",
    "                                       axis=1\n",
    "                                      )\n",
    "\n",
    "blast_test_pred = blast_model.predict(x_test_blast_enriched)[:, 0]\n",
    "evaluate_percent(y_blast_test, blast_test_pred, name=\"Blast (test)\")\n",
    "\n",
    "# Evaluate on HOLDOUT set (completely unseen)\n",
    "print(\"\\nEvaluating on final holdout set...\")\n",
    "fert_holdout_pred = fert_model.predict(x_holdout_scaled)[:, 0]\n",
    "evaluate_percent(y_fert_holdout, fert_holdout_pred, name=\"Fert (holdout)\")\n",
    "\n",
    "x_holdout_blast_enriched = np.concatenate([x_holdout_scaled , fert_holdout_pred.reshape(-1,1)],\n",
    "                                          axis=1\n",
    "                                         )\n",
    "\n",
    "\n",
    "blast_holdout_pred = blast_model.predict(x_holdout_blast_enriched)[:, 0]\n",
    "evaluate_percent(y_blast_holdout, blast_holdout_pred, name=\"Blast (holdout)\")\n",
    "\n",
    "\n",
    "# Comparison plots for validation vs. predictions\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(y_fert_val * 100, fert_model.predict(x_val_scaled)[:,0]*100, alpha=0.6)\n",
    "plt.xlabel(\"True Fertilization (%)\")\n",
    "plt.ylabel(\"Predicted Fertilization (%)\")\n",
    "plt.title(\"Validation Fertilization\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_blast_val * 100, blast_model.predict(x_val_blast_enriched)[:,0]*100, alpha=0.6)\n",
    "plt.xlabel(\"True Blastulation (%)\")\n",
    "plt.ylabel(\"Predicted Blastulation (%)\")\n",
    "plt.title(\"Validation Blastulation\")\n",
    "plt.show()\n",
    "\n",
    "# Save trained models\n",
    "fert_model.save(\"final_fertilization_model.keras\")\n",
    "blast_model.save(\"final_blastulation_model.keras\")\n",
    "\n",
    "# Save scaler for future inference\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "\n",
    "print(\"✓ Models and scaler successfully saved for deployment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70703c7-0500-4cc1-9666-9a90451a0655",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rates(new_data_df):\n",
    "    \"\"\"\n",
    "    Input: new_data_df (pandas DataFrame with same columns as x_train)\n",
    "    Output: dict with Fertilization% and Blastulation%\n",
    "    \"\"\"\n",
    "    # Load artifacts if needed\n",
    "    from tensorflow.keras.models import load_model\n",
    "    import joblib\n",
    "    import numpy as np\n",
    "    from scipy.special import expit\n",
    "\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    fert_model = load_model(\"final_fertilization_model.keras\", custom_objects={\"beta_nll_loss\": beta_nll_loss, \"mae_mu\": mae_mu, \"OutputScaler\": OutputScaler})\n",
    "    blast_model = load_model(\"final_blastulation_model.keras\", custom_objects={\"beta_nll_loss\": beta_nll_loss, \"mae_mu\": mae_mu, \"OutputScaler\": OutputScaler})\n",
    "\n",
    "    # Scale new inputs\n",
    "    new_scaled = scaler.transform(new_data_df)\n",
    "\n",
    "    # Predict\n",
    "    \n",
    "    fert_pred_mu = fert_model.predict(new_scaled)[:,0]\n",
    "    x_blast_enriched= np.concatenate([new_scaled, fert_pred_mu.reshape(-1,1)], axis=1)\n",
    "    \n",
    "    blast_pred_mu = blast_model.predict(x_blast_enriched)[:,0]\n",
    "\n",
    "    fert_pred= fert_pred_mu*100\n",
    "    blast_pred = blast_pred_mu*100\n",
    "    \n",
    "    return {\n",
    "        \"Fertilization_Rate(%)\": np.mean(fert_pred),\n",
    "        \"Blastulation_Rate(%)\": np.mean(blast_pred)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff85f58-6a01-441c-8b9a-cc259025d6bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
